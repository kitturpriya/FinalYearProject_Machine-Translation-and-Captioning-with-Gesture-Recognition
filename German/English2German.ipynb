{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import load,dump\n",
    "\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'deu.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(n_units)))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2404\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3856\n",
      "German Max Length: 10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 256)           987136    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 512)               1050624   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 5, 512)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 5, 512)            1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 5, 2404)           1233252   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4845924 (18.49 MB)\n",
      "Trainable params: 4845924 (18.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.51618, saving model to English2German.h5\n",
      "141/141 - 15s - loss: 4.1652 - val_loss: 3.5162 - 15s/epoch - 108ms/step\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 3.51618 to 3.15598, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 3.2870 - val_loss: 3.1560 - 9s/epoch - 64ms/step\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss improved from 3.15598 to 2.86638, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 2.8320 - val_loss: 2.8664 - 9s/epoch - 65ms/step\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss improved from 2.86638 to 2.68299, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 2.4447 - val_loss: 2.6830 - 9s/epoch - 66ms/step\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss improved from 2.68299 to 2.50870, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 2.1109 - val_loss: 2.5087 - 9s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss improved from 2.50870 to 2.37203, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 1.7942 - val_loss: 2.3720 - 9s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss improved from 2.37203 to 2.25995, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 1.5055 - val_loss: 2.2600 - 9s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: val_loss improved from 2.25995 to 2.16176, saving model to English2German.h5\n",
      "141/141 - 9s - loss: 1.2357 - val_loss: 2.1618 - 9s/epoch - 67ms/step\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: val_loss improved from 2.16176 to 2.07465, saving model to English2German.h5\n",
      "141/141 - 11s - loss: 1.0039 - val_loss: 2.0746 - 11s/epoch - 78ms/step\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: val_loss improved from 2.07465 to 2.01219, saving model to English2German.h5\n",
      "141/141 - 11s - loss: 0.8089 - val_loss: 2.0122 - 11s/epoch - 76ms/step\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: val_loss improved from 2.01219 to 2.00716, saving model to English2German.h5\n",
      "141/141 - 11s - loss: 0.6536 - val_loss: 2.0072 - 11s/epoch - 77ms/step\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: val_loss improved from 2.00716 to 1.98386, saving model to English2German.h5\n",
      "141/141 - 12s - loss: 0.5287 - val_loss: 1.9839 - 12s/epoch - 86ms/step\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: val_loss improved from 1.98386 to 1.97928, saving model to English2German.h5\n",
      "141/141 - 11s - loss: 0.4294 - val_loss: 1.9793 - 11s/epoch - 78ms/step\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.97928\n",
      "141/141 - 11s - loss: 0.3626 - val_loss: 1.9913 - 11s/epoch - 76ms/step\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: val_loss improved from 1.97928 to 1.96974, saving model to English2German.h5\n",
      "141/141 - 11s - loss: 0.3031 - val_loss: 1.9697 - 11s/epoch - 79ms/step\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.96974\n",
      "141/141 - 11s - loss: 0.2613 - val_loss: 1.9972 - 11s/epoch - 80ms/step\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.96974\n",
      "141/141 - 10s - loss: 0.2303 - val_loss: 1.9966 - 10s/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.96974\n",
      "141/141 - 11s - loss: 0.2023 - val_loss: 2.0154 - 11s/epoch - 75ms/step\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.96974\n",
      "141/141 - 11s - loss: 0.1839 - val_loss: 2.0562 - 11s/epoch - 81ms/step\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.96974\n",
      "141/141 - 12s - loss: 0.1678 - val_loss: 2.0339 - 12s/epoch - 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x107813bd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='English2German.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'English2German.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[es ist nicht dumm], target=[its not stupid], predicted=[its not stupid]\n",
      "src=[ich wunsche ihnen einen schonen tag], target=[have a nice day], predicted=[have a nice day]\n",
      "src=[vertraust du ihr], target=[do you trust her], predicted=[do you trust her]\n",
      "src=[ich nehme geschenke an], target=[i accept gifts], predicted=[i accept gifts]\n",
      "src=[ich habe ein pferd], target=[i have a horse], predicted=[i have a horse]\n",
      "src=[zeig ihn uns], target=[show it to us], predicted=[show it to us]\n",
      "src=[er ist ein komodiant], target=[hes a comedian], predicted=[hes a comedian]\n",
      "src=[geht und wartet drauen], target=[go wait outside], predicted=[go wait outside]\n",
      "src=[pack dich], target=[go away], predicted=[get away]\n",
      "src=[gehort das hier euch], target=[is this yours], predicted=[is this yours]\n",
      "BLEU-1: 0.945676\n",
      "BLEU-2: 0.926988\n",
      "BLEU-3: 0.871600\n",
      "BLEU-4: 0.639888\n",
      "test\n",
      "src=[alle waren sich einig], target=[everyone agreed], predicted=[everybody finished]\n",
      "src=[mir geht es schlecht], target=[im not well], predicted=[i feel bad]\n",
      "src=[rate mal], target=[try to guess], predicted=[i i tell tell]\n",
      "src=[ich werde es bezahlen], target=[ill pay for it], predicted=[ill pay later]\n",
      "src=[komm mich besuchen], target=[come and see me], predicted=[come and see me]\n",
      "src=[liebst du mich], target=[do you love me], predicted=[do you love me]\n",
      "src=[ich hasse heuchelei], target=[i hate hypocrisy], predicted=[i hate karaoke]\n",
      "src=[tom fuhlte sich krank], target=[tom felt sick], predicted=[tom felt ill]\n",
      "src=[benimm dich wie ein mann], target=[act like a man], predicted=[what are a man]\n",
      "src=[pack dich], target=[get lost], predicted=[get away]\n",
      "BLEU-1: 0.580840\n",
      "BLEU-2: 0.457779\n",
      "BLEU-3: 0.383130\n",
      "BLEU-4: 0.220181\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook English2German.ipynb to html\n",
      "[NbConvertApp] Writing 327602 bytes to English2German.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html 'English2German.ipynb'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
